{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Multi-label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import other useful packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Load the Yeast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1 -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2  0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3  0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4  0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.041850  0.066938 -0.056617  ...       0       0       1       1       0   \n",
       "1 -0.077933 -0.080529 -0.016267  ...       0       0       0       0       0   \n",
       "2  0.013646 -0.040666 -0.024447  ...       0       0       0       0       0   \n",
       "3 -0.007670  0.079438  0.062184  ...       0       0       0       0       0   \n",
       "4  0.064456 -0.133387  0.068878  ...       1       1       0       0       0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0        0        0        1        1        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        1        1        0  \n",
       "3        0        0        0        0        0  \n",
       "4        0        0        0        0        0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your code here\n",
    "dataset = pd.read_csv('yeast.csv')\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2417, 117)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Att1       0\n",
       "Att2       0\n",
       "Att3       0\n",
       "Att4       0\n",
       "Att5       0\n",
       "          ..\n",
       "Class10    0\n",
       "Class11    0\n",
       "Class12    0\n",
       "Class13    0\n",
       "Class14    0\n",
       "Length: 117, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values, there is no missing values in this dataset\n",
    "print(\"Missing Values\")\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[dataset.columns[0:103]]\n",
    "Y = np.array(dataset[dataset.columns[103:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size = 0.30, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the Binary Relevance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class BinaryRelevance(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    #constructor for the classifier object, default model is SVC\n",
    "    def __init__(self, classifier = SVC()):\n",
    "        self.classifier = classifier\n",
    "     \n",
    "    #train classifier, create a classifier for each model\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.classifiers_= []\n",
    "        self._label_count = y.shape[1]\n",
    "        self.label_ = list(range(y.shape[1]))\n",
    "        for i in range(self._label_count):\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            y_subset = y[:,i]\n",
    "            classifier.fit(X,y_subset)\n",
    "            self.classifiers_.append(classifier)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    #use the classifers that create during the train to make predition \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = np.zeros([X.shape[0],self._label_count],dtype=np.int)\n",
    "        for label in range(self._label_count):\n",
    "            predictions[:,label] = self.classifiers_[label].predict(X)\n",
    "        return predictions\n",
    "         \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        result = np.zeros([X.shape[0],self._label_count], dtype='float')\n",
    "        for label_assignment, classifier in zip(self.label_, self.classifiers_):\n",
    "            result[:, label_assignment] = classifier.predict_proba(X)[:, 1]\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=DecisionTreeClassifier(class_weight=None,\n",
       "                                                  criterion='entropy',\n",
       "                                                  max_depth=None,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=10,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  presort=False,\n",
       "                                                  random_state=None,\n",
       "                                                  splitter='best'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = BinaryRelevance(tree.DecisionTreeClassifier(criterion='entropy',min_samples_leaf=10))\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=10, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14 classifiers in total\n",
    "my_model.classifiers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 1, 0, ..., 1, 1, 0],\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88235294, 0.        , 0.47368421, ..., 1.        , 0.84210526,\n",
       "        0.        ],\n",
       "       [0.        , 0.55555556, 0.1875    , ..., 0.9375    , 1.        ,\n",
       "        0.        ],\n",
       "       [0.86666667, 1.        , 0.30769231, ..., 1.        , 1.        ,\n",
       "        0.1       ],\n",
       "       ...,\n",
       "       [0.5       , 1.        , 0.        , ..., 1.        , 0.75      ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.8       , ..., 0.8       , 0.        ,\n",
       "        0.07142857],\n",
       "       [0.5       , 0.26315789, 0.        , ..., 0.05882353, 0.3125    ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = my_model.predict_proba(X_test)\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement the Binary Relevance Algorithm with Under-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class BinaryRelevanceWithResampling(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    #constructor for the classifier object\n",
    "    def __init__(self, classifier = SVC(), under_sampling = True):\n",
    "        self.classifier = classifier\n",
    "        self.under_sampling = under_sampling\n",
    "    \n",
    "    #train classifier, create a classifier for each model,under-sampling all X and each y\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.classifiers_= []\n",
    "        self._label_count = y.shape[1]\n",
    "        self.label_ = list(range(y.shape[1]))\n",
    "        \n",
    "        for i in range(self._label_count):\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            X_resampled = X\n",
    "            y_resampled = y[:,i]\n",
    "            if(self.under_sampling):\n",
    "                rus = RandomUnderSampler(random_state=2020)\n",
    "                X_resampled, y_resampled = rus.fit_sample(X_resampled, y_resampled)\n",
    "            classifier.fit(X_resampled,y_resampled)\n",
    "            self.classifiers_.append(classifier)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        predictions = np.zeros([X.shape[0],self._label_count],dtype=np.int)\n",
    "        for label in range(self._label_count):\n",
    "            predictions[:,label] = self.classifiers_[label].predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        result = np.zeros([X.shape[0],self._label_count], dtype='float')\n",
    "        for label_assignment, classifier in zip(self.label_, self.classifiers_):\n",
    "            result[:, label_assignment] = classifier.predict_proba(X)[:, 1]\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceWithResampling(classifier=SVC(C=1.0, cache_size=200,\n",
       "                                             class_weight=None, coef0=0.0,\n",
       "                                             decision_function_shape='ovr',\n",
       "                                             degree=3, gamma='auto_deprecated',\n",
       "                                             kernel='rbf', max_iter=-1,\n",
       "                                             probability=True,\n",
       "                                             random_state=None, shrinking=True,\n",
       "                                             tol=0.001, verbose=False),\n",
       "                              under_sampling=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set parameter probability=True for SVC predict_proba\n",
    "my_model = BinaryRelevanceWithResampling(SVC(probability=True))\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 1],\n",
       "       [0, 1, 1, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56595594, 0.44311136, 0.27509489, ..., 0.52749454, 0.52341669,\n",
       "        0.53699457],\n",
       "       [0.31418826, 0.2998366 , 0.56444155, ..., 0.53029676, 0.52201696,\n",
       "        0.52959052],\n",
       "       [0.55392834, 0.36275746, 0.18171631, ..., 0.52228667, 0.52030165,\n",
       "        0.53434461],\n",
       "       ...,\n",
       "       [0.63545146, 0.5       , 0.0870271 , ..., 0.51347719, 0.51202427,\n",
       "        0.54090636],\n",
       "       [0.41596636, 0.60552626, 0.5       , ..., 0.5       , 0.5       ,\n",
       "        0.52796494],\n",
       "       [0.39760679, 0.50731808, 0.73223041, ..., 0.48464408, 0.48690314,\n",
       "        0.52459955]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = my_model.predict_proba(X_test)\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Compare the Performance of Different Binary Relevance Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                               coef0=0.0, decision_function_shape='ovr',\n",
       "                               degree=3, gamma='auto_deprecated', kernel='rbf',\n",
       "                               max_iter=-1, probability=False,\n",
       "                               random_state=None, shrinking=True, tol=0.001,\n",
       "                               verbose=False))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "my_model = BinaryRelevance()\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1515313275511919"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceWithResampling(classifier=SVC(C=1.0, cache_size=200,\n",
       "                                             class_weight=None, coef0=0.0,\n",
       "                                             decision_function_shape='ovr',\n",
       "                                             degree=3, gamma='auto_deprecated',\n",
       "                                             kernel='rbf', max_iter=-1,\n",
       "                                             probability=False,\n",
       "                                             random_state=None, shrinking=True,\n",
       "                                             tol=0.001, verbose=False),\n",
       "                              under_sampling=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = BinaryRelevanceWithResampling()\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 1],\n",
       "       [0, 1, 1, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41199454814010406"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisonTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=DecisionTreeClassifier(class_weight=None,\n",
       "                                                  criterion='gini',\n",
       "                                                  max_depth=None,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  presort=False,\n",
       "                                                  random_state=None,\n",
       "                                                  splitter='best'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = BinaryRelevance(tree.DecisionTreeClassifier())\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3856360464409511"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceWithResampling(classifier=DecisionTreeClassifier(class_weight=None,\n",
       "                                                                criterion='gini',\n",
       "                                                                max_depth=None,\n",
       "                                                                max_features=None,\n",
       "                                                                max_leaf_nodes=None,\n",
       "                                                                min_impurity_decrease=0.0,\n",
       "                                                                min_impurity_split=None,\n",
       "                                                                min_samples_leaf=1,\n",
       "                                                                min_samples_split=2,\n",
       "                                                                min_weight_fraction_leaf=0.0,\n",
       "                                                                presort=False,\n",
       "                                                                random_state=None,\n",
       "                                                                splitter='best'),\n",
       "                              under_sampling=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = BinaryRelevanceWithResampling(tree.DecisionTreeClassifier())\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 1, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 1],\n",
       "       [1, 0, 1, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41289697153754007"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                              dual=False, fit_intercept=True,\n",
       "                                              intercept_scaling=1,\n",
       "                                              l1_ratio=None, max_iter=100,\n",
       "                                              multi_class='warn', n_jobs=None,\n",
       "                                              penalty='l2', random_state=2020,\n",
       "                                              solver='warn', tol=0.0001,\n",
       "                                              verbose=0, warm_start=False))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = BinaryRelevance(LogisticRegression(random_state=2020))\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3363784317668851"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevanceWithResampling(classifier=LogisticRegression(C=1.0,\n",
       "                                                            class_weight=None,\n",
       "                                                            dual=False,\n",
       "                                                            fit_intercept=True,\n",
       "                                                            intercept_scaling=1,\n",
       "                                                            l1_ratio=None,\n",
       "                                                            max_iter=100,\n",
       "                                                            multi_class='warn',\n",
       "                                                            n_jobs=None,\n",
       "                                                            penalty='l2',\n",
       "                                                            random_state=2020,\n",
       "                                                            solver='warn',\n",
       "                                                            tol=0.0001,\n",
       "                                                            verbose=0,\n",
       "                                                            warm_start=False),\n",
       "                              under_sampling=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = BinaryRelevanceWithResampling(LogisticRegression(random_state=2020))\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44053920793095835"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class ClassifierChain(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    #constructor for the classifier object\n",
    "    def __init__(self, classifier = SVC(), order = None):\n",
    "        self.classifier = classifier\n",
    "        self.order = order\n",
    "    \n",
    "    #train classifier, after each y be used combine it with X, use new X dataset and next y train   \n",
    "    def fit(self, X, y, order = None):\n",
    "        \n",
    "        X_temp = X\n",
    "        self._label_count = y.shape[1]\n",
    "        self.classifiers_ = [None for x in range(self._label_count)]\n",
    "        \n",
    "        if(self.order is None):\n",
    "            self.order_ = list(range(self._label_count))\n",
    "        else:\n",
    "            self.order_ = self.order\n",
    "        \n",
    "        for label in self.order_:\n",
    "            self.classifier = copy.deepcopy(self.classifier)\n",
    "            y_subset = y[:,label]\n",
    "            self.classifiers_[label] = self.classifier.fit(X_temp, y_subset)\n",
    "            X_temp = np.column_stack((X_temp, y_subset))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    #use the classifers that create during the train to make predition\n",
    "    #the last _label_count columns in x are the labels that predicted\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X_temp = X\n",
    "        \n",
    "        for label in self.order_:\n",
    "            prediction = self.classifiers_[label].predict(X_temp)\n",
    "            X_temp = np.column_stack((X_temp, prediction))\n",
    "            \n",
    "        return X_temp[:,-self._label_count:].astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        X_temp = X\n",
    "        proba = []\n",
    "        \n",
    "        for label in self.order_:\n",
    "            prediction = self.classifiers_[label].predict(X_temp)\n",
    "            prediction_proba = self.classifiers_[label].predict_proba(X_temp)[:, 1]\n",
    "            X_temp = np.column_stack((X_temp, prediction))\n",
    "            \n",
    "        proba.append(prediction_proba)\n",
    "        \n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                               coef0=0.0, decision_function_shape='ovr',\n",
       "                               degree=3, gamma='auto_deprecated', kernel='rbf',\n",
       "                               max_iter=-1, probability=True, random_state=None,\n",
       "                               shrinking=True, tol=0.001, verbose=False),\n",
       "                order=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = ClassifierChain(SVC(probability=True))\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00554066, 0.0062078 , 0.00411866, 0.00992003, 0.00174294,\n",
       "        0.00425465, 0.00696259, 0.00348581, 0.00441725, 0.012424  ,\n",
       "        0.00389347, 0.00513095, 0.00490546, 0.002363  , 0.00354636,\n",
       "        0.00428867, 0.00205866, 0.00409301, 0.0038408 , 0.00749712,\n",
       "        0.00356184, 0.00404392, 0.00540702, 0.00322106, 0.00667238,\n",
       "        0.00510923, 0.00872554, 0.00383181, 0.01429821, 0.00679696,\n",
       "        0.0026579 , 0.0054603 , 0.00470578, 0.00562098, 0.00603201,\n",
       "        0.00784397, 0.00498093, 0.02125224, 0.00878044, 0.00611914,\n",
       "        0.00477153, 0.00291235, 0.00664552, 0.00314877, 0.00497616,\n",
       "        0.01140059, 0.00629231, 0.00406809, 0.00260006, 0.00713641,\n",
       "        0.00638752, 0.00408564, 0.00474677, 0.01494761, 0.00551679,\n",
       "        0.00355627, 0.00246327, 0.0122709 , 0.00809326, 0.00327685,\n",
       "        0.00752649, 0.00302533, 0.00444142, 0.00606553, 0.00646647,\n",
       "        0.01512597, 0.00635295, 0.00327263, 0.00282627, 0.00287178,\n",
       "        0.00592062, 0.00420672, 0.00305759, 0.00482605, 0.00491652,\n",
       "        0.00197276, 0.00572111, 0.00219415, 0.00591671, 0.00297317,\n",
       "        0.01574461, 0.00852809, 0.00608591, 0.00563789, 0.0102418 ,\n",
       "        0.01071087, 0.00515993, 0.00840228, 0.00689477, 0.00890504,\n",
       "        0.00692983, 0.0033807 , 0.0070884 , 0.00737987, 0.00365025,\n",
       "        0.00712293, 0.00250481, 0.00501285, 0.00746012, 0.00499552,\n",
       "        0.01179469, 0.00409839, 0.00402864, 0.00508729, 0.01171026,\n",
       "        0.00274238, 0.00157623, 0.00258496, 0.01062235, 0.00193309,\n",
       "        0.00333793, 0.0049106 , 0.00881856, 0.00450344, 0.00474889,\n",
       "        0.01119496, 0.00258055, 0.00480899, 0.00567833, 0.01064835,\n",
       "        0.0060385 , 0.00453519, 0.00181396, 0.00336303, 0.00730361,\n",
       "        0.00554658, 0.0090507 , 0.00454892, 0.00571602, 0.0048152 ,\n",
       "        0.00526433, 0.00193573, 0.00794466, 0.00209613, 0.00198427,\n",
       "        0.00731653, 0.00392788, 0.01005906, 0.00596227, 0.00417472,\n",
       "        0.00709457, 0.01008228, 0.00522693, 0.00382033, 0.00400288,\n",
       "        0.00173121, 0.00717349, 0.00617111, 0.0044504 , 0.00135115,\n",
       "        0.00507088, 0.00730897, 0.00654167, 0.00959561, 0.01054415,\n",
       "        0.00315454, 0.00934394, 0.00094   , 0.0084733 , 0.00487637,\n",
       "        0.01025982, 0.00237927, 0.00452767, 0.00452046, 0.01018484,\n",
       "        0.02369365, 0.00421931, 0.00432758, 0.00576936, 0.00327138,\n",
       "        0.00538032, 0.00387708, 0.00555205, 0.00092134, 0.00335517,\n",
       "        0.00439351, 0.00486141, 0.00351528, 0.00792807, 0.00385824,\n",
       "        0.00531702, 0.00910966, 0.00371721, 0.00490921, 0.00244989,\n",
       "        0.00373175, 0.0045325 , 0.00457916, 0.00170351, 0.00421619,\n",
       "        0.00377261, 0.00735106, 0.00130369, 0.0032843 , 0.00540307,\n",
       "        0.0025198 , 0.00773487, 0.00141621, 0.00722705, 0.01040148,\n",
       "        0.00667011, 0.01252036, 0.01463295, 0.00494569, 0.00085885,\n",
       "        0.00758047, 0.00152084, 0.00261424, 0.00272422, 0.00819368,\n",
       "        0.00653715, 0.00549922, 0.00733542, 0.00733577, 0.00292225,\n",
       "        0.00258881, 0.0072131 , 0.00951325, 0.00931063, 0.00268641,\n",
       "        0.00594574, 0.00638921, 0.01610353, 0.00802523, 0.00270523,\n",
       "        0.0064658 , 0.00522942, 0.00438689, 0.00785435, 0.01090326,\n",
       "        0.01718332, 0.00484016, 0.00970945, 0.010004  , 0.00372361,\n",
       "        0.00932103, 0.00329464, 0.00639176, 0.00934698, 0.00803466,\n",
       "        0.00664978, 0.00737124, 0.00966681, 0.00357022, 0.0088672 ,\n",
       "        0.00209427, 0.00591889, 0.00226226, 0.0018525 , 0.00149048,\n",
       "        0.00317447, 0.0047509 , 0.00256748, 0.00523607, 0.00267876,\n",
       "        0.00441386, 0.00337298, 0.00327817, 0.00887017, 0.00850502,\n",
       "        0.00434402, 0.0088372 , 0.00860164, 0.00283275, 0.02051188,\n",
       "        0.00453569, 0.0046979 , 0.00377497, 0.00700458, 0.00764804,\n",
       "        0.00460033, 0.00362708, 0.00350119, 0.00373624, 0.005385  ,\n",
       "        0.00170845, 0.00512066, 0.00465483, 0.00486488, 0.00376397,\n",
       "        0.01176305, 0.01549701, 0.01207545, 0.00288309, 0.01092629,\n",
       "        0.00799604, 0.03131951, 0.01625537, 0.01176136, 0.00277037,\n",
       "        0.00384473, 0.0146337 , 0.00391666, 0.00774765, 0.00683014,\n",
       "        0.00270707, 0.0106104 , 0.00290582, 0.00751685, 0.00388205,\n",
       "        0.00790742, 0.00329665, 0.0033988 , 0.00229121, 0.00148868,\n",
       "        0.00530977, 0.0064823 , 0.00413272, 0.00671337, 0.03275905,\n",
       "        0.00498782, 0.00577132, 0.00571229, 0.004442  , 0.01016928,\n",
       "        0.00379037, 0.00466065, 0.00280543, 0.00706606, 0.01223332,\n",
       "        0.00243236, 0.00620187, 0.00521578, 0.00643367, 0.00612429,\n",
       "        0.00343425, 0.00426457, 0.00303958, 0.00801439, 0.00787745,\n",
       "        0.00575288, 0.00157258, 0.00976673, 0.00547367, 0.00632732,\n",
       "        0.00938379, 0.00582971, 0.00583281, 0.0086    , 0.0032203 ,\n",
       "        0.00939791, 0.00413822, 0.00654681, 0.00349454, 0.00509162,\n",
       "        0.00595566, 0.00700206, 0.00403552, 0.01257252, 0.0046133 ,\n",
       "        0.00324933, 0.00951426, 0.00371368, 0.0066474 , 0.00285337,\n",
       "        0.01207455, 0.00252039, 0.00718255, 0.01470278, 0.00158212,\n",
       "        0.00888236, 0.00590647, 0.00369665, 0.00631157, 0.0027052 ,\n",
       "        0.00220532, 0.00749862, 0.00416708, 0.0028024 , 0.00456658,\n",
       "        0.00251495, 0.00345727, 0.00411359, 0.00342219, 0.00731845,\n",
       "        0.00661774, 0.00964466, 0.0037159 , 0.00177615, 0.00582503,\n",
       "        0.01466037, 0.00827126, 0.00477788, 0.00203039, 0.003172  ,\n",
       "        0.00589821, 0.00245051, 0.00501917, 0.00914491, 0.00497381,\n",
       "        0.00580342, 0.00245269, 0.01816013, 0.00547137, 0.00628973,\n",
       "        0.00684481, 0.01075962, 0.00508244, 0.00211436, 0.00727626,\n",
       "        0.00712938, 0.00370753, 0.00394068, 0.02233526, 0.00390153,\n",
       "        0.00791514, 0.00225699, 0.0059923 , 0.00745474, 0.00755678,\n",
       "        0.00902836, 0.00342528, 0.00215089, 0.00547877, 0.00302252,\n",
       "        0.00551988, 0.0065419 , 0.00482425, 0.00407697, 0.00530469,\n",
       "        0.00443283, 0.00244235, 0.00940235, 0.0076094 , 0.00517149,\n",
       "        0.00959982, 0.00168211, 0.00393415, 0.00393138, 0.01282602,\n",
       "        0.00245743, 0.00657856, 0.00947348, 0.00338579, 0.00561002,\n",
       "        0.00348338, 0.00799687, 0.00940146, 0.00552367, 0.00209898,\n",
       "        0.00161499, 0.01002807, 0.00333541, 0.00292433, 0.00616085,\n",
       "        0.02143371, 0.00989715, 0.00366969, 0.01702093, 0.00323164,\n",
       "        0.00117729, 0.00224515, 0.00488676, 0.00345847, 0.00282409,\n",
       "        0.00756966, 0.00295212, 0.00425392, 0.00329226, 0.00382129,\n",
       "        0.00630357, 0.00673658, 0.00610914, 0.00966406, 0.00156329,\n",
       "        0.00566102, 0.00937874, 0.00469564, 0.00295598, 0.00381984,\n",
       "        0.00287711, 0.00518908, 0.00187464, 0.00340659, 0.00253412,\n",
       "        0.00393183, 0.00860087, 0.00607615, 0.01154429, 0.00885435,\n",
       "        0.00600946, 0.00245613, 0.00450789, 0.0150976 , 0.00764201,\n",
       "        0.00836799, 0.00712259, 0.00474515, 0.00962034, 0.00372818,\n",
       "        0.00659113, 0.00459285, 0.02659283, 0.00553256, 0.00860485,\n",
       "        0.00829419, 0.00611745, 0.0037267 , 0.00257673, 0.01278946,\n",
       "        0.00725476, 0.00307832, 0.00314439, 0.00451847, 0.00748721,\n",
       "        0.00848533, 0.00279292, 0.00420241, 0.00696665, 0.00161326,\n",
       "        0.00827102, 0.00533591, 0.00484267, 0.00281896, 0.004042  ,\n",
       "        0.00520623, 0.02878956, 0.00367908, 0.00622561, 0.00561517,\n",
       "        0.00516958, 0.00467686, 0.00507707, 0.00525416, 0.00239367,\n",
       "        0.00217685, 0.00640092, 0.01107856, 0.00503476, 0.01227398,\n",
       "        0.00345207, 0.00755091, 0.00204196, 0.00614424, 0.01073836,\n",
       "        0.00215848, 0.00385373, 0.00936614, 0.00266121, 0.00760254,\n",
       "        0.00203341, 0.00948944, 0.00344009, 0.01230218, 0.00440554,\n",
       "        0.0041877 , 0.0069824 , 0.0065998 , 0.01141102, 0.00761473,\n",
       "        0.00370379, 0.02273007, 0.00242839, 0.00416735, 0.00248916,\n",
       "        0.00449432, 0.00492592, 0.00466768, 0.00852107, 0.00604562,\n",
       "        0.00519911, 0.00564886, 0.00808292, 0.00355567, 0.01630478,\n",
       "        0.00284391, 0.0029298 , 0.01530984, 0.00619828, 0.0019751 ,\n",
       "        0.00449654, 0.0082413 , 0.01222893, 0.00634646, 0.00699997,\n",
       "        0.00936821, 0.00417549, 0.0079793 , 0.00597514, 0.00323904,\n",
       "        0.00532751, 0.00437736, 0.0045103 , 0.01095787, 0.00311756,\n",
       "        0.01275653, 0.02597913, 0.00360203, 0.02022334, 0.00801838,\n",
       "        0.00252807, 0.00367399, 0.0058355 , 0.00205443, 0.00309388,\n",
       "        0.00447332, 0.00755647, 0.00788839, 0.00349209, 0.00586582,\n",
       "        0.00365819, 0.01346686, 0.01232705, 0.0041954 , 0.00163336,\n",
       "        0.00284972, 0.00719686, 0.00751841, 0.0113475 , 0.0168669 ,\n",
       "        0.00491322, 0.00463102, 0.00513446, 0.01377469, 0.00486861,\n",
       "        0.00288317, 0.00405978, 0.00418012, 0.00580813, 0.00286204,\n",
       "        0.00980995, 0.00445475, 0.00153349, 0.00341526, 0.01177339,\n",
       "        0.00572791, 0.00487523, 0.00370325, 0.0042884 , 0.00214299,\n",
       "        0.00286725, 0.00366173, 0.00236794, 0.00480836, 0.00878338,\n",
       "        0.00631398, 0.0027845 , 0.00290963, 0.00335383, 0.00308853,\n",
       "        0.01856639, 0.01339524, 0.00273003, 0.0045194 , 0.00579175,\n",
       "        0.00361007, 0.00898802, 0.00612226, 0.016323  , 0.00569945,\n",
       "        0.00336259, 0.00464468, 0.00363593, 0.00438567, 0.00812081,\n",
       "        0.00601328, 0.00288351, 0.01102506, 0.00311282, 0.0023719 ,\n",
       "        0.01387912, 0.01264908, 0.03080572, 0.00769174, 0.01038824,\n",
       "        0.00673115, 0.00428686, 0.00818892, 0.00656686, 0.01502974,\n",
       "        0.00469162, 0.00279596, 0.00887375, 0.00540845, 0.01100428,\n",
       "        0.00594365, 0.00431351, 0.00533106, 0.00529806, 0.00988079,\n",
       "        0.0147386 , 0.0008219 , 0.00718557, 0.01187617, 0.0035876 ,\n",
       "        0.00244495, 0.01041097, 0.00490932, 0.00875978, 0.00328804,\n",
       "        0.00811889, 0.0099568 , 0.00821554, 0.00148091, 0.025762  ,\n",
       "        0.00300997, 0.00128686, 0.00617791, 0.0026068 , 0.00142334,\n",
       "        0.00520339, 0.00297397, 0.00278007, 0.00236668, 0.00427664,\n",
       "        0.00631703, 0.0045463 , 0.00395715, 0.01065349, 0.00265652,\n",
       "        0.0084038 , 0.00622226, 0.00971733, 0.00564422, 0.00305667,\n",
       "        0.00102682, 0.00635232, 0.00869169, 0.00395775, 0.00366502,\n",
       "        0.00702654, 0.00486515, 0.00626193, 0.00211307, 0.01219821,\n",
       "        0.00779992])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba = my_model.predict_proba(X_test)\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate the Performance of the Classifier Chains Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                               coef0=0.0, decision_function_shape='ovr',\n",
       "                               degree=3, gamma='auto_deprecated', kernel='rbf',\n",
       "                               max_iter=-1, probability=False,\n",
       "                               random_state=None, shrinking=True, tol=0.001,\n",
       "                               verbose=False),\n",
       "                order=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "my_model = ClassifierChain()\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14486146718765816"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=DecisionTreeClassifier(class_weight=None,\n",
       "                                                  criterion='gini',\n",
       "                                                  max_depth=None,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  presort=False,\n",
       "                                                  random_state=None,\n",
       "                                                  splitter='best'),\n",
       "                order=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = ClassifierChain(tree.DecisionTreeClassifier())\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3871600644877598"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                              dual=False, fit_intercept=True,\n",
       "                                              intercept_scaling=1,\n",
       "                                              l1_ratio=None, max_iter=100,\n",
       "                                              multi_class='warn', n_jobs=None,\n",
       "                                              penalty='l2', random_state=2020,\n",
       "                                              solver='warn', tol=0.0001,\n",
       "                                              verbose=0, warm_start=False),\n",
       "                order=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = ClassifierChain(LogisticRegression(random_state=2020))\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3842887858153895"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find a label order to Classifier Chain to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.75      0.48      0.59       238\n",
      "     class 1       0.58      0.41      0.48       321\n",
      "     class 2       0.58      0.58      0.58       283\n",
      "     class 3       0.62      0.56      0.59       244\n",
      "     class 4       0.67      0.39      0.49       217\n",
      "     class 5       0.50      0.16      0.25       176\n",
      "     class 6       0.67      0.02      0.03       115\n",
      "     class 7       0.25      0.01      0.01       139\n",
      "     class 8       0.00      0.00      0.00        58\n",
      "     class 9       0.00      0.00      0.00        75\n",
      "    class 10       0.50      0.01      0.02        80\n",
      "    class 11       0.73      0.96      0.83       532\n",
      "    class 12       0.73      0.95      0.82       527\n",
      "    class 13       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.68      0.56      0.61      3015\n",
      "   macro avg       0.47      0.32      0.34      3015\n",
      "weighted avg       0.61      0.56      0.54      3015\n",
      " samples avg       0.68      0.57      0.59      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_model = BinaryRelevance(LogisticRegression(random_state=2020))\n",
    "my_model.fit(X_train, y_train)\n",
    "y_pred = my_model.predict(X_test)\n",
    "target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5','class 6', 'class 7', 'class 8',\n",
    "                'class 9', 'class 10', 'class 11','class 12', 'class 13']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(classifier=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                              dual=False, fit_intercept=True,\n",
       "                                              intercept_scaling=1,\n",
       "                                              l1_ratio=None, max_iter=100,\n",
       "                                              multi_class='warn', n_jobs=None,\n",
       "                                              penalty='l2', random_state=2020,\n",
       "                                              solver='warn', tol=0.0001,\n",
       "                                              verbose=0, warm_start=False),\n",
       "                order=[11, 12, 2, 3, 0, 4, 1, 5, 6, 7, 8, 9, 10, 13])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = [11,12,2,3,0,4,1,5,6,7,8,9,10,13]\n",
    "my_model = ClassifierChain(LogisticRegression(random_state=2020),order)\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24885631450336673"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "metrics.f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.33      0.96      0.49       238\n",
      "     class 1       0.44      0.96      0.60       321\n",
      "     class 2       0.59      0.63      0.61       283\n",
      "     class 3       0.55      0.62      0.58       244\n",
      "     class 4       0.12      0.10      0.11       217\n",
      "     class 5       0.30      0.34      0.32       176\n",
      "     class 6       0.19      0.41      0.26       115\n",
      "     class 7       0.28      0.28      0.28       139\n",
      "     class 8       0.11      0.12      0.12        58\n",
      "     class 9       0.10      0.08      0.09        75\n",
      "    class 10       0.00      0.00      0.00        80\n",
      "    class 11       0.80      0.01      0.01       532\n",
      "    class 12       0.80      0.01      0.02       527\n",
      "    class 13       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.37      0.35      0.36      3015\n",
      "   macro avg       0.33      0.32      0.25      3015\n",
      "weighted avg       0.50      0.35      0.27      3015\n",
      " samples avg       0.37      0.39      0.36      3015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your reflection here (max 300 words)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Daniels, Z.A. and Metaxas, D.N.(2017), the class imbalance is a serious issue in multilabel classification. For this issue macro averaged F-score evaluation methods be used. Macro averaged F-score is the arithmetic mean of F-score and F-score is for balance between precision and recall. \n",
    "\n",
    "Compare BinaryRelevance and BinaryRelevanceWithResampling:\n",
    "\n",
    "|          SVC                   | Macro averaged F1  |\n",
    "|  ----------------------------  | -----------------  |\n",
    "| BinaryRelevance                |      0.151         |\n",
    "| BinaryRelevanceWithResampling  |      0.411         |\n",
    "\n",
    "\n",
    "\n",
    "|        Decision Tree           | Macro averaged F1  |\n",
    "|  ----------------------------  | -----------------  |\n",
    "| BinaryRelevance                |      0.385         |\n",
    "| BinaryRelevanceWithResampling  |      0.409         |\n",
    "\n",
    "\n",
    "|        LogisticRegression      | Macro averaged F1  |\n",
    "|  ----------------------------  | -----------------  |\n",
    "| BinaryRelevance                |      0.336         |\n",
    "| BinaryRelevanceWithResampling  |      0.440         |\n",
    "\n",
    "\n",
    "The Macro averaged F1 of BinaryRelevanceWithResampling obviously higher than BinaryRelevance. We expect high macro averaged F1 score. I use:\n",
    "```\n",
    "start = time.time()\n",
    "      ...\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "```\n",
    "measured the training time complexity of two approaches. BinaryRevance used 2.549s and BinaryRelevanceWithResampling used 1.297s. BinaryRelevanceWithResampling is much faster than the BinaryRevance method. In my opinion, this is because of the resampled dataset smaller than the whole dataset.\n",
    "\n",
    "Compare BinaryRelevance, BinaryRelevanceWithResampling and ClassifierChain:\n",
    "\n",
    "|          SVC                   | Macro averaged F1  | \n",
    "|  ----------------------------  | -----------------  |\n",
    "| BinaryRelevance                |      0.151         |\n",
    "| BinaryRelevanceWithResampling  |      0.411         |\n",
    "| ClassifierChain                |      0.144         |\n",
    "\n",
    "|        Decision Tree           | Macro averaged F1  |\n",
    "|  ----------------------------  | -----------------  |\n",
    "| BinaryRelevance                |      0.385         |\n",
    "| BinaryRelevanceWithResampling  |      0.409         |\n",
    "| ClassifierChain                |      0.389         |\n",
    "\n",
    "|        LogisticRegression      | Macro averaged F1  |\n",
    "|  ----------------------------  | -----------------  |\n",
    "| BinaryRelevance                |      0.336         |\n",
    "| BinaryRelevanceWithResampling  |      0.440         |\n",
    "| ClassifierChain                |      0.384         |\n",
    "\n",
    "Use the decision tree model. Sometimes I got the score of ClassifierChain higher than BinaryRelevance. I find decision tree model macro averaged f1-score always change, maybe because every time training the dataset has a slight change, decision tree a little bit unstable. \n",
    "\n",
    "ClassifierChain performance should be better than BinaryRelevance. Classifier chains considering the relevance of labels, using predictions of some labels as inputs in later label predictions. It worse than the under-sampling BinaryRelevance. The training time of ClassifierChain is 2.477s shorter than BinaryRelevance and longer than under-sampling BinaryRelevance.\n",
    "\n",
    "In the experiment, I use the default label order. Compare with this order or random order some specific chain order may improve the accuracy of ClassifierChain. I used classification_report to get f1-score for each label and use the order of f1-score from high to low but as a result, I got lower macro averaged f1-score. Maybe when that label independent, it has high accuracy. When associating them, each label be influenced by the previous label, the accuracy decrease. I may do some research about the label order next.\n",
    "\n",
    "Reference:\n",
    "Daniels, Z.A. and Metaxas, D.N., 2017, February. Addressing imbalance in multi-label classification using structured hellinger forests. In Thirty-First AAAI Conference on Artificial Intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
